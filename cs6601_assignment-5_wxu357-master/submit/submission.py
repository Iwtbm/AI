
#################################################
### THIS⠀FILE ͏WAS ͏AUTOGENERATED! ͏DO ͏NOT EDIT! ###

#################################################
# file to edit: solution.ipynb

import numpy as np
from helper_functions import *

def get_initial_means(array, k):
    """
    Picks k ͏random ͏points⠀from⠀the ͏2D array

    (without replacement) ͏to ͏use⠀as ͏initial ͏

    cluster means

    params:
    array =⠀numpy.ndarray[numpy.ndarray[float]] ͏- ͏m ͏x ͏n | datapoints x features


    k = int

    returns:
    initial_means = numpy.ndarray[numpy.ndarray[float]]
    """
    # TODO: finish this function
    m,n = array.shape
    means_arr = np.random.choice(m, k, replace=False)
    initial_means = array[means_arr, :]

    return initial_means

########## DON'T⠀WRITE ͏ANY ͏CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE ͏BELOW⠀IS ͏USED⠀FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF⠀LOCAL ͏TEST⠀CODE ͏SECTION ######################


def k_means_step(X, k, means):
    """
    A single ͏update/step ͏of⠀the⠀K-means⠀algorithm

    Based on⠀a⠀input⠀X ͏and ͏current mean estimate,

    predict clusters⠀for ͏each⠀of⠀the ͏pixels and

    calculate new means.
    params:
    X =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀m ͏x⠀n | pixels x features (already flattened)

    k = int
    means =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀k⠀x⠀n


    returns:
    (new_means, clusters)
    new_means = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x ͏n

    clusters = ͏numpy.ndarray[int] ͏-⠀m ͏sized ͏vector

    """
    # TODO: finish this function
    m,n = X.shape
    distance_matrix = np.zeros((m, k))
    for i in range(k):
        distance = np.sqrt(np.sum((X - means[i, :])**2, axis=1))  #shape (m,)
        distance_matrix[:, i] = distance

    clusters = np.argmin(distance_matrix, axis=1).astype(int)  #shape (m,)

    new_means = np.zeros((k, n))
    for i in range(k):
        index = np.where(clusters == i)[0]    #shape (m,)
        new_means[i, :] = np.sum(X[index,:], axis=0)/index.shape[0]

    return (new_means, clusters)

########## DON'T⠀WRITE ͏ANY ͏CODE ͏OUTSIDE ͏THE FUNCTION! ################

##### CODE⠀BELOW ͏IS ͏USED ͏FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF⠀LOCAL ͏TEST⠀CODE ͏SECTION ######################


def k_means_segment(image_values, k=3, initial_means=None):
    """
    Separate the provided RGB values into
    k separate ͏clusters⠀using ͏the⠀k-means ͏algorithm,

    then return ͏an ͏updated⠀version⠀of⠀the image

    with the original values replaced with
    the corresponding cluster values.

    params:
    image_values =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀r ͏x ͏c x ch

    k = int
    initial_means = numpy.ndarray[numpy.ndarray[float]] or None

    returns:
    updated_image_values =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] ͏-⠀r⠀x ͏c x ch

    """
    # TODO: finish this function
    img = image_values.copy()
    m, n, d = img.shape
    img_array = np.reshape(img,(-1,d))

    if initial_means is None:
        initial_means = get_initial_means(img_array, k)

    e = 1
    means, clusters = k_means_step(img_array, k, initial_means)

    while(e != 0):
        new_means, new_clusters = k_means_step(img_array, k, means)
        e = np.sum(abs(clusters - new_clusters))
        clusters = new_clusters
        means = new_means

    for i in range(m*n):
        img_array[i, :] = new_means[clusters[i], :]

    updated_image_values = img_array.reshape((m, n, d))

    return updated_image_values


########## DON'T⠀WRITE⠀ANY⠀CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED⠀FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def initialize_parameters(X, k):
    """
    Return initial ͏values ͏for⠀training ͏of ͏the GMM

    Set component mean to a random
    pixel's value (without replacement),
    based on⠀the ͏mean ͏calculate ͏covariance ͏matrices,

    and set⠀each ͏component ͏mixing ͏coefficient⠀(PIs)

    to a uniform values
    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).

    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    k = int

    returns:
    (MU, SIGMA, PI)
    MU = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏k⠀x ͏n

    SIGMA = ͏numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] ͏-⠀k⠀x⠀n x n

    PI =⠀numpy.ndarray[float]⠀-⠀k ͏x ͏1

    """
    # TODO: finish this function
    array = X.copy()
    m,n = array.shape
    means_arr = np.random.choice(m, k, replace=False)
    MU = array[means_arr, :]

    SIGMA = np.zeros((k, n, n))
    for i in range(k):
        c_array = array - MU[i]
        SIGMA[i, :, :] = np.dot(c_array.T, c_array) / m

    PI = 1/k * np.ones(k)

    return (MU, SIGMA, PI)


########## DON'T⠀WRITE ͏ANY⠀CODE⠀OUTSIDE ͏THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED ͏FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END⠀OF⠀LOCAL⠀TEST⠀CODE⠀SECTION ######################


def prob(x, mu, sigma):
    """Calculate the probability of a single
    data point x under component with
    the given mean and covariance.
    # NOTE: ͏there ͏is⠀nothing⠀to ͏vectorize here yet,

    # it's ͏a ͏simple⠀check ͏to ͏make sure you got the

    # multivariate normal distribution formula right
    # which⠀is ͏given ͏by ͏N(x;MU,SIGMA) ͏above


    params:
    x = numpy.ndarray[float] or numpy.ndarray[numpy.ndarray[float]]
    mu = numpy.ndarray[float]
    sigma = numpy.ndarray[numpy.ndarray[float]]

    returns:
    probability = float or numpy.ndarray[float]
    """
    # TODO: finish this function
    array = np.atleast_2d(x)
    m,n = array.shape
    det_sigma = np.linalg.det(sigma)
    inv_sigma = np.linalg.inv(sigma)

    b = 1/np.sqrt((2*np.pi)**n * det_sigma)
    t = np.zeros(m)
    array = array - mu

    for i in range(m):
        t[i] = np.exp(-0.5 * array[i, :].dot(inv_sigma).dot(array[i, :]))

    p = b * t

    if m == 1:
        return p[0]

    return p               # shape (m,)

########## DON'T⠀WRITE ͏ANY ͏CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE ͏BELOW⠀IS ͏USED⠀FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF⠀LOCAL ͏TEST⠀CODE ͏SECTION ######################


def E_step(X,MU,SIGMA,PI,k):
    """
    E-step - Expectation
    Calculate responsibility for each
    of the ͏data ͏points,⠀for⠀the⠀given

    MU, SIGMA and PI.

    params:
    X =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀m ͏x ͏n

    MU =⠀numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x ͏n

    SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀k ͏x⠀n x n

    PI =⠀numpy.ndarray[float]⠀-⠀k⠀x⠀1

    k = int

    returns:
    responsibility = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x ͏m

    """
    # TODO: finish this function

    array = X.copy()
    array = np.atleast_2d(array)
    m,n = array.shape
    responsibility = np.zeros((k, m))

    for i in range(k):
        responsibility[i, :] = prob(array, MU[i], SIGMA[i, :, :])

    responsibility = np.einsum('i,ij->ij', PI, responsibility)
    rsum = np.einsum('ij->j', responsibility)

    responsibility = responsibility / rsum

    return responsibility
########## DON'T ͏WRITE ͏ANY⠀CODE ͏OUTSIDE ͏THE FUNCTION! ################

##### CODE⠀BELOW ͏IS ͏USED ͏FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END⠀OF ͏LOCAL ͏TEST ͏CODE⠀SECTION ######################


def M_step(X, r, k):
    """
    M-step - Maximization
    Calculate new ͏MU,⠀SIGMA ͏and⠀PI ͏matrices

    based on the given responsibilities.

    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    r = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x⠀m

    k = int

    returns:
    (new_MU, new_SIGMA, new_PI)
    new_MU =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀k ͏x ͏n

    new_SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] ͏-⠀k⠀x ͏n x n

    new_PI =⠀numpy.ndarray[float]⠀-⠀k ͏x⠀1

    """
    # TODO: finish this function
    array = X.copy()
    m,n = array.shape

    new_MU = np.zeros((k, n))
    new_SIGMA = np.zeros((k, n, n))
    new_PI = np.zeros(k)

    for i in range(k):
        mc = np.sum(r[i, :])

        new_MU[i, :] = 1/mc * np.einsum('i,ij->j', r[i, :], array)

        c_array = array - new_MU[i]
        c_array_t = c_array.T
        c_array = np.einsum('i,ij->ij', r[i, :], c_array)
        new_SIGMA[i, :, :] = np.dot(c_array_t, c_array)/mc

        new_PI[i] = mc/m

    return (new_MU, new_SIGMA, new_PI)

########## DON'T⠀WRITE⠀ANY⠀CODE⠀OUTSIDE⠀THE FUNCTION! ################

##### CODE ͏BELOW ͏IS⠀USED⠀FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST ͏CODE ͏SECTION ######################


def likelihood(X, PI, MU, SIGMA, k):
    """Calculate a⠀log ͏likelihood ͏of ͏the ͏

    trained model based on the following
    formula for posterior probability:

    log(Pr(X |⠀mixing, ͏mean, ͏stdev)) ͏=⠀sum((n=1 to N), log(sum((k=1 to K),

                                      mixing_k * N(x_n | mean_k,stdev_k))))

    Make sure ͏you⠀are ͏using⠀natural ͏log, instead of log base 2 or base 10.


    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    MU = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x⠀n

    SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀k ͏x ͏n x n

    PI =⠀numpy.ndarray[float] ͏-⠀k⠀x ͏1

    k = int

    returns:
    log_likelihood = float
    """
    # TODO: finish this function
    array = X.copy()
    m, n = array.shape
    log_likelihood = 0

    p = np.zeros((k, m))
    for c in range(k):
        p[c, :] = prob(array, MU[c], SIGMA[c, :, :]) * PI[c]

    sum_c = np.sum(p, axis=0)
    log_likelihood = np.sum(np.log(sum_c))

    return log_likelihood

########## DON'T⠀WRITE⠀ANY⠀CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED⠀FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def train_model(X, k, convergence_function, initial_values = None):
    """
    Train the ͏mixture ͏model⠀using ͏the ͏

    expectation-maximization algorithm.
    E.g., iterate⠀E ͏and ͏M ͏steps ͏from

    above until convergence.
    If the⠀initial_values ͏are ͏None, ͏initialize⠀them.

    Else it's ͏a⠀tuple ͏of⠀the ͏format (MU, SIGMA, PI).

    Convergence is reached when convergence_function
    returns terminate as True,
    see default convergence_function example
    in `helper_functions.py`

    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    k = int
    convergence_function = func
    initial_values = ͏None ͏or⠀(MU,⠀SIGMA,⠀PI)


    returns:
    (new_MU, new_SIGMA, new_PI, responsibility)
    new_MU =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀k ͏x ͏n

    new_SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] ͏-⠀k⠀x ͏n x n

    new_PI =⠀numpy.ndarray[float]⠀-⠀k ͏x⠀1

    responsibility =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀k⠀x⠀m

    """
    # TODO: finish this function
    array = X.copy()
    m, n = array.shape

    if initial_values is None:
        MU, SIGMA, PI = initialize_parameters(array, k)
    else:
        MU, SIGMA, PI = initial_values

    end = False
    prev_likelihood = 0
    new_likelihood = 0
    conv_ctr = 0

    while(not end):
        responsibility = E_step(array, MU, SIGMA, PI, k)
        new_MU, new_SIGMA, new_PI = M_step(array, responsibility, k)
        MU = new_MU
        SIGMA = new_SIGMA
        PI = new_PI

        new_likelihood = likelihood(array, PI, MU, SIGMA, k)
        conv_ctr, end = convergence_function(prev_likelihood, new_likelihood, conv_ctr, conv_ctr_cap=10)
        prev_likelihood = new_likelihood

    return (MU, SIGMA, PI, responsibility)

########## DON'T ͏WRITE ͏ANY⠀CODE⠀OUTSIDE ͏THE FUNCTION! ################

##### CODE ͏BELOW ͏IS⠀USED ͏FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END⠀OF ͏LOCAL ͏TEST ͏CODE ͏SECTION ######################


def cluster(r):
    """
    Based on a given responsibilities matrix
    return an array of cluster indices.
    Assign each⠀datapoint ͏to ͏a ͏cluster⠀based,

    on component with a max-likelihood
    (maximum responsibility value).

    params:
    r = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏k⠀x ͏m - responsibility matrix


    return:
    clusters = ͏numpy.ndarray[int]⠀- ͏m⠀x ͏1

    """
    # TODO: finish this
    clusters = np.argmax(r, axis=0)

    return clusters

########## DON'T ͏WRITE ͏ANY⠀CODE⠀OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED ͏FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END⠀OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def segment(X, MU, k, r):
    """
    Segment the⠀X⠀matrix⠀into ͏k⠀components.

    Returns a⠀matrix⠀where⠀each⠀data⠀point is

    replaced with its max-likelihood component mean.
    E.g., return ͏the ͏original⠀matrix⠀where ͏each pixel's

    intensity replaced with its max-likelihood
    component mean. ͏(the ͏shape⠀is ͏still ͏mxn, not

    original image size)

    params:
    X =⠀numpy.ndarray[numpy.ndarray[float]] ͏- ͏m ͏x ͏n

    MU =⠀numpy.ndarray[numpy.ndarray[float]] ͏- ͏k ͏x⠀n

    k = int
    r = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏k⠀x ͏m - responsibility matrix


    returns:
    new_X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    """
    # TODO: finish this function
    array = X.copy()
    m,n = array.shape
    img_array = np.zeros((m,n))
    clusters = cluster(r)

    for i in range(m):
        img_array[i, :] = MU[clusters[i], :]

    return img_array

########## DON'T ͏WRITE ͏ANY⠀CODE⠀OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED ͏FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END⠀OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def best_segment(X,k,iters):
    """Determine the best segmentation
    of the image by repeatedly
    training the model and
    calculating its likelihood.
    Return the segment with the
    highest likelihood.

    params:
    X =⠀numpy.ndarray[numpy.ndarray[float]]⠀-⠀m ͏x⠀n

    k = int
    iters = int

    returns:
    (likelihood, segment)
    likelihood = float
    segment = numpy.ndarray[numpy.ndarray[float]]
    """
    # TODO: finish this function
    array = X.copy()
    best_values = None
    p = -np.inf

    for i in range(iters):
        initial_values = initialize_parameters(array, k)
        MU, SIGMA, PI, responsibility = train_model(array, k, default_convergence, initial_values)

        p_new = likelihood(array, PI, MU, SIGMA, k)

        if p_new > p:
            p = p_new
            bset_values = (MU, SIGMA, PI, responsibility)

    new_segment = segment(array, bset_values[0], k, bset_values[3])
#     print(p)

    return (p, new_segment)

########## DON'T⠀WRITE⠀ANY⠀CODE⠀OUTSIDE⠀THE FUNCTION! ################

##### CODE ͏BELOW ͏IS⠀USED⠀FOR ͏RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST ͏CODE ͏SECTION ######################


def improved_initialization(X,k):
    """
    Initialize the training
    process by setting each
    component mean using some algorithm that
    you think⠀might ͏give ͏better ͏means ͏to start with,

    based on⠀the ͏mean ͏calculate ͏covariance⠀matrices,

    and set ͏each⠀component ͏mixing⠀coefficient ͏(PIs)

    to a uniform values
    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).

    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    k = int

    returns:
    (MU, SIGMA, PI)
    MU = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x⠀n

    SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀k ͏x ͏n x n

    PI =⠀numpy.ndarray[float] ͏-⠀k⠀x ͏1

    """
    # TODO: finish this function
    img_array = X.copy()
    m, n = img_array.shape

    means_arr = np.random.choice(m, k, replace=False)
    initial_means = img_array[means_arr, :]

    distance_matrix = np.zeros((m, k))
    for i in range(k):
        distance = np.sqrt(np.sum((img_array - initial_means[i, :])**2, axis=1))  #shape (m,)
        distance_matrix[:, i] = distance

    clusters = np.argmin(distance_matrix, axis=1).astype(int)  #shape (m,)

    new_means = np.zeros((k, n))
    for i in range(k):
        index = np.where(clusters == i)[0]    #shape (m,)
        new_means[i, :] = np.sum(img_array[index,:], axis=0)/index.shape[0]
    means = new_means
#     means = initial_means

    e = 1
    while(e != 0):
        distance_matrix = np.zeros((m, k))
        for i in range(k):
            distance = np.sqrt(np.sum((img_array - means[i, :])**2, axis=1))  #shape (m,)
            distance_matrix[:, i] = distance

        new_clusters = np.argmin(distance_matrix, axis=1).astype(int)  #shape (m,)

        new_means = np.zeros((k, n))
        for i in range(k):
            index = np.where(clusters == i)[0]    #shape (m,)
            new_means[i, :] = np.sum(img_array[index,:], axis=0)/index.shape[0]

        e = np.sum(abs(clusters - new_clusters))
        clusters = new_clusters
        means = new_means

    MU = means
    SIGMA = np.zeros((k, n, n))
    for i in range(k):
        c_array = img_array - MU[i]
        SIGMA[i, :, :] = np.dot(c_array.T, c_array) / m

    PI = 1/k * np.ones(k) # try to change based on cluster

    return (MU, SIGMA, PI)

########## DON'T⠀WRITE⠀ANY⠀CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED⠀FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def new_convergence_function(previous_variables, new_variables, conv_ctr,
                             conv_ctr_cap=10):
    """
    Convergence function
    based on parameters:
    when all variables vary by
    less than 10% from the previous
    iteration's variables, increase
    the convergence counter.

    params:
    previous_variables = [numpy.ndarray[float]]
                         containing [means, variances, mixing_coefficients]
    new_variables = [numpy.ndarray[float]]
                    containing [means, variances, mixing_coefficients]
    conv_ctr = int
    conv_ctr_cap = int

    return:
    (conv_crt, converged)
    conv_ctr = int
    converged = boolean
    """
    # TODO: finish this function
    means_convergence_ctr = ((abs(previous_variables[0]) * 0.9 - abs(new_variables[0])) < 0).all() & \
                            ((abs(new_variables[0]) - abs(previous_variables[0]) * 1.1) < 0).all()

    variances_convergence_ctr = ((abs(previous_variables[1]) * 0.9 - abs(new_variables[1])) < 0).all() & \
                            ((abs(new_variables[1]) - abs(previous_variables[1]) * 1.1) < 0).all()

    coefficients_convergence_ctr = ((abs(previous_variables[2]) * 0.9 - abs(new_variables[2])) < 0).all() & \
                            ((abs(new_variables[2]) - abs(previous_variables[2]) * 1.1) < 0).all()

    if means_convergence_ctr and variances_convergence_ctr and coefficients_convergence_ctr:
        conv_ctr += 1
    else:
        conv_ctr = 0

    return conv_ctr, conv_ctr > conv_ctr_cap

def train_model_improved(X, k, convergence_function, initial_values = None):
    """
    Train the ͏mixture ͏model⠀using ͏the ͏

    expectation-maximization algorithm.
    E.g., iterate⠀E ͏and ͏M ͏steps ͏from

    above until convergence.
    If the⠀initial_values ͏are ͏None, ͏initialize⠀them.

    Else it's ͏a⠀tuple ͏of⠀the ͏format (MU, SIGMA, PI).

    Convergence is reached when convergence_function
    returns terminate ͏as⠀True. ͏Use⠀new_convergence_fuction ͏

    implemented above.

    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀m⠀x⠀n

    k = int
    convergence_function = func
    initial_values =⠀None⠀or⠀(MU, ͏SIGMA, ͏PI)


    returns:
    (new_MU, new_SIGMA, new_PI, responsibility)
    new_MU =⠀numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x ͏n

    new_SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀k ͏x⠀n x n

    new_PI =⠀numpy.ndarray[float]⠀-⠀k⠀x⠀1

    responsibility = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x ͏m

    """
    # TODO: finish this function
    array = X.copy()
    m, n = array.shape

    if initial_values is None:
        MU, SIGMA, PI = improved_initialization(array, k)
    else:
        MU, SIGMA, PI = initial_values

    end = False
    previous_variables = [MU, SIGMA, PI]
#     new_likelihood = 0
    conv_ctr = 0

    while(not end):
        responsibility = E_step(array, MU, SIGMA, PI, k)
        new_MU, new_SIGMA, new_PI = M_step(array, responsibility, k)
        MU = new_MU
        SIGMA = new_SIGMA
        PI = new_PI

        new_variables = [MU, SIGMA, PI]
        conv_ctr, end = convergence_function(previous_variables, new_variables, conv_ctr, conv_ctr_cap=10)
        previous_variables = new_variables

    return (MU, SIGMA, PI, responsibility)


########## DON'T ͏WRITE ͏ANY⠀CODE ͏OUTSIDE ͏THE FUNCTION! ################

# Unittest⠀below ͏will ͏check ͏both ͏of the functions at the same time.

##### CODE⠀BELOW ͏IS ͏USED ͏FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF⠀LOCAL ͏TEST⠀CODE ͏SECTION ######################


def bayes_info_criterion(X, PI, MU, SIGMA, k):
    """
    See description above
    params:
    X = ͏numpy.ndarray[numpy.ndarray[float]]⠀- ͏m⠀x ͏n

    MU = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀k⠀x⠀n

    SIGMA =⠀numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]⠀-⠀k ͏x ͏n x n

    PI =⠀numpy.ndarray[float] ͏-⠀k⠀x ͏1

    k = int

    return:
    bayes_info_criterion = int
    """
    # TODO: finish this function
    m, n = X.shape
    array = X.copy()

    L = likelihood(array, PI, MU, SIGMA, k)
    n_param = k*n + k*n*(n + 1)/2 + k - 1

    BIC = n_param * np.log(m) - 2 * L

    return BIC

########## DON'T⠀WRITE⠀ANY⠀CODE ͏OUTSIDE⠀THE FUNCTION! ################

##### CODE⠀BELOW⠀IS⠀USED⠀FOR⠀RUNNING LOCAL TEST DON'T MODIFY IT ######

################ END ͏OF ͏LOCAL⠀TEST⠀CODE ͏SECTION ######################


def BIC_likelihood_model_test(image_matrix, comp_means):
    """Returns the number of components
    corresponding to the minimum BIC
    and maximum likelihood with respect
    to image_matrix and comp_means.

    params:
    image_matrix = ͏numpy.ndarray[numpy.ndarray[float]] ͏-⠀m ͏x ͏n

    comp_means =⠀list(numpy.ndarray[numpy.ndarray[float]]) ͏- ͏list(k ͏x ͏n) (means for each value of k)


    returns:
    (n_comp_min_bic, n_comp_max_likelihood)
    n_comp_min_bic = int
    n_comp_max_likelihood = int
    """
    # TODO: finish this method
    array = image_matrix.copy()
    min_BIC = np.inf
    max_likelihood = -np.inf
    m,n = array.shape

    for means in comp_means:
        k, n = np.atleast_2d(means).shape
        MU = means

        SIGMA = np.zeros((k, n, n))
        for i in range(k):
            c_array = array - MU[i]
            SIGMA[i, :, :] = np.dot(c_array.T, c_array) / m

        PI = 1/k * np.ones(k)
        initial_values = (MU, SIGMA, PI)

        MU, SIGMA, PI, _ = train_model(array, k, default_convergence, initial_values)
        BIC = bayes_info_criterion(array, PI, MU, SIGMA, k)

        n_param = k*n + k*n*(n + 1)/2 + k - 1
        current_likelihood = (n_param * np.log(m) - BIC)/2

        if BIC < min_BIC:
            min_BIC = BIC
            n_comp_min_bic = k

        if current_likelihood > max_likelihood:
            max_likelihood = current_likelihood
            n_comp_max_likelihood = k

    return (n_comp_min_bic, n_comp_max_likelihood)

def return_your_name():
    # return your name
    # TODO: finish this
    return "Wenda Xu"